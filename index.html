<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title>reveal-md</title>
    <link rel="shortcut icon" href="./favicon.ico" />
    <link rel="stylesheet" href="./dist/reveal.css" />
    <link rel="stylesheet" href="./dist/theme/black.css" id="theme" />
    <link rel="stylesheet" href="./css/highlight/zenburn.css" />


  </head>
  <body>
    <div class="reveal">
      <div class="slides"><section  data-markdown><script type="text/template"><style>
.reveal h1,
.reveal h2,
.reveal h3,
.reveal h4,
.reveal h5,
.reveal h6 {
  text-transform: none;
}
</style>

# Introduction to Hydra
</script></section><section  data-markdown><script type="text/template">
# Code of conduct
We are operating under the Carpentries [Code of Conduct](https://docs.carpentries.org/topic_folders/policies/code-of-conduct.html).

If you feel that someone has violated this Code of Conduct, please email `si-hpc@si.edu`.
</script></section><section  data-markdown><script type="text/template">
# Introductions
</script></section><section  data-markdown><script type="text/template">
# Intended outcomes
After attending this workshop, we hope users come away with these skills:

* How to successfully log in
* How to submit a job
* What to do if something doesn't work
* How to work responsibly on a shared computing resource
</script></section><section  data-markdown><script type="text/template">
# Hydra (SI/HPC)
</script></section><section  data-markdown><script type="text/template">
### People

* Rebecca Dikow (OCIO Data Science Lab), Vanessa Gonz√°lez (NMNH GGI), Matt Kweskin (NMNH LAB), and Mike Trizna (OCIO Data Science Lab) provide support for non-CfA users.

* DJ Ding (OCIO) is the full-time Hydra system administrator.
* Sylvain Korzennik (SAO) is the HPC Analyst and provides support for CfA users.
</script></section><section  data-markdown><script type="text/template">
### Getting help
* The [Wiki](https://confluence.si.edu/display/HPC/High+Performance+Computing) contains detailed documentation
* Email `si-hpc-admin@si.edu` for system-level issues
* For non-CfA users:
	* 	Bioinformatics Brown Bag (Wednesdays, 12-1pm ET, on Zoom)
	* Email `si-hpc@si.edu` (monitored by Rebecca, Vanessa, Matt, and Mike)
* CfA users:
	* email Sylvain or sign up for his office hours

</script></section><section  data-markdown><script type="text/template">
### Being a good Hydra citizen
* We strive to provide support for users that is inclusive, welcoming, and helps you get your science done.

* We request that users be respectful when asking for help. While we attempt to answer questions rapidly, user support is no one's full-time duties.
</script></section><section  data-markdown><script type="text/template">
### How is a cluster different than a single-user system?


![](https://i.imgur.com/nf1YzbQ.jpg)

</script></section><section  data-markdown><script type="text/template">
### How is a cluster different than a single-user system?
* Hydra has 90 compute nodes with between 20 and 128 CPUs each, for a total of 4,896 CPUs

* Compute nodes have a range of 128GB to 2TB RAM each

</script></section><section  data-markdown><script type="text/template">

### Important Takeaways

* Users never need to connect to the Head Node

* Log in to either `hydra-login01` or `hydra-login02`

* Do not run commands that use substantial CPU on the login nodes, that's what the compute nodes are for

</script></section><section  data-markdown><script type="text/template">
### Disk Storage

* When you log in, you go to your `/home` directory

* `/home` is for your own installed programs and scripts, not for data storage

* Data belong on `/pool` or `/scratch` and users should run their jobs from here

* `/pool` and `/scratch` are scrubbed - files older than 180 days are removed
</script></section><section  data-markdown><script type="text/template">
### Connecting to Hydra

* telework.si.edu (web terminal)
* Mac direct connect (onsite or VPN)
* Windows direct connect (onsite, remote desktop, VPN)
* CfA (telework.si.edu, login.cfa.harvard.edu, SAO VPN)

* If you don't have an SI VPN but would like to, there is a request form in the SI ServiceDesk
</script></section><section  data-markdown><script type="text/template">
### The job scheduler - UGE

* We use UGE (Univa Grid Engine) to schedule resources on Hydra

* When you submit a job, UGE adds it to the queue and sends it to a compute node with the resources you request

* Each job is assigned a JOB ID, which you can use to check on progress and look at how it used resources when it is complete
</script></section><section  data-markdown><script type="text/template">
### Submitting jobs

* The most common way to run analysis on Hydra is by submitting a job file using the command `qsub`

* We will show you how to build a job file in just a bit

* Users can also start an interactive session using `qrsh`
</script></section><section  data-markdown><script type="text/template">
### Queues
Hydra has different queues to accommodate different resource requests:
* High CPU queues: `sThC.q`, `mThC.q`, `lThC.q`, `uThC.q`
* High Memory queues: `sThM.q`, `mThM.q`, `lThM.q`, `uThM.q`

There are other more specialized queues, check the wiki for more information
</script></section><section  data-markdown><script type="text/template">
### Parallelization
* Depending on the software, you may be able to run a job in **parallel**, which can speed up your analysis.
* Some software uses **threaded** parallelization, where the job is divided across CPUs on a single compute node
* Some software can be compiled to use **MPI** parallelization, where the job is divided across multiple compute nodes


Look at software documentation to check which kind of parallelization, if any, your software uses
</script></section><section  data-markdown><script type="text/template">
### Parallelization hints
* Some (bioinformatics) software will grab all the CPUs on a compute node unless you tell it otherwise

* Best practice is to use `$NSLOTS` in place of a number of threads in your command. We will demo this in a bit
</script></section><section  data-markdown><script type="text/template">
### Warnings

* Users that are:
    * Running a job that is inefficient (using <30% of the requested CPU resources), or
    * Running a high-memory job that is using much less than the requested amount of RAM,

*will receive an automated warning email. We request that you monitor these jobs closely and contact us if you receive repeated warnings*
</script></section><section  data-markdown><script type="text/template">
# Let's Connect
</script></section><section  data-markdown><script type="text/template"></script></section></div>
    </div>

    <script src="./dist/reveal.js"></script>

    <script src="./plugin/markdown/markdown.js"></script>
    <script src="./plugin/highlight/highlight.js"></script>
    <script src="./plugin/zoom/zoom.js"></script>
    <script src="./plugin/notes/notes.js"></script>
    <script src="./plugin/math/math.js"></script>
    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }

      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        plugins: [
          RevealMarkdown,
          RevealHighlight,
          RevealZoom,
          RevealNotes,
          RevealMath
        ]
      };

      // options from URL query string
      var queryOptions = Reveal().getQueryHash() || {};

      var options = extend(defaultOptions, {}, queryOptions);
    </script>


    <script>
      Reveal.initialize(options);
    </script>
  </body>
</html>
